{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede67a78",
   "metadata": {
    "height": 30
   },
   "source": [
    "# JAX Code in python for classical cnn, using MNIST dataset 50 size train and 30 size test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8302bf1",
   "metadata": {
    "height": 1118
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random, grad, jit\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import Conv, Relu, MaxPool, Flatten, Dense, LogSoftmax\n",
    "\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.nn import softmax_cross_entropy\n",
    "\n",
    "from jax.experimental.stax import serial\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = one_hot(y_train, 10), one_hot(y_test, 10)\n",
    "    return x_train[:50], y_train[:50], x_test[:30], y_test[:30]\n",
    "\n",
    "def loss(params, batch):\n",
    "    inputs, targets = batch\n",
    "    preds = predict(params, inputs)\n",
    "    return jnp.mean(softmax_cross_entropy(targets, preds))\n",
    "\n",
    "def accuracy(params, batch):\n",
    "    inputs, targets = batch\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "init_random_params, predict = serial(Conv(32, (5, 5), (1, 1), 'SAME'),\n",
    "                                     Relu,\n",
    "                                     MaxPool((2, 2), (2, 2), 'VALID'),\n",
    "                                     Conv(64, (5, 5), (1, 1), 'SAME'),\n",
    "                                     Relu,\n",
    "                                     MaxPool((2, 2), (2, 2), 'VALID'),\n",
    "                                     Flatten,\n",
    "                                     Dense(1024),\n",
    "                                     Relu,\n",
    "                                     Dense(10),\n",
    "                                     LogSoftmax)\n",
    "\n",
    "_, initial_params = init_random_params(random.PRNGKey(0), (-1, 28, 28, 1))\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(1e-3)\n",
    "opt_state = opt_init(initial_params)\n",
    "\n",
    "@jit\n",
    "def update(i, opt_state, batch):\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, grad(loss)(params, batch), opt_state)\n",
    "\n",
    "x_train, y_train, x_test, y_test = data()\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(0, len(x_train), 10):\n",
    "        batch = (x_train[j:j+10], y_train[j:j+10])\n",
    "        opt_state = update(i, opt_state, batch)\n",
    "    train_acc = accuracy(get_params(opt_state), (x_train, y_train))\n",
    "    test_acc = accuracy(get_params(opt_state), (x_test, y_test))\n",
    "    print(f\"Step {i}, Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
